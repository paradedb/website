import Image from "next/image";
import { ReferenceLine } from "recharts";
import { BarChart, Card, Subtitle, Bold } from "@tremor/react"
import FacetingChart from "./FacetingChart"

import blogMetadata from "./metadata.json";
import { AuthorSection } from "@/components/AuthorSection";
import { HeroImage } from "@/components/HeroImage";
import { Title } from "@/components/Title";
import { Note } from "@mintlify/components"
import faceting from "./images/demandsciencesmall.png";

<Title metadata={blogMetadata} />
<AuthorSection metadata={blogMetadata} />
<HeroImage metadata={blogMetadata} />

Picture this: you've built a perfectly reasonable search app. Users type "laptop," get results, everybody's happy. Then someone asks, "but how many of these are Dell laptops?" So you add filters. Then they want to know how many results each filter would give them *before* they click it. Welcome to faceted search.

If you're thinking, "I'll just run a few extra COUNT queries, how hard can it be?", you're about to discover why search engines exist in the first place.

The thing is, faceting *looks* simple: it's just grouping and counting. But try to make it fast in a traditional row-based database, and you'll run into serious performance challenges. Want to show search results *and* category counts from a single query? That's either two index scans or a full index scan and a lot of data transferred. Want it to be responsive? That becomes increasingly difficult as the number of results increases.
 
In this post, we'll break down how we brought **Elasticsearch-style faceting directly into PostgreSQL;** the syntax, the planner integration, and the execution strategy that makes it possible. We'll show you how we structured the SQL API using window functions, and the results we can get from pushing as much work as possible down into our underlying search library (spoiler alert: over an order of magnitude faster for large result sets).

<Note>
If you aren’t familiar with [ParadeDB](https://paradedb.com), we bring the performance and developer experience of Elasticsearch into Postgres.
</Note>

## How Search Engines Count

Faceting is a way of summarizing the contents of a search result. When you type a query like “laptop,” you’re not just asking for a list of matching documents, you’re also thinking, *what kinds of laptops exist in this result set?* Maybe there are 1,200 Dell laptops, 800 Lenovo, and 600 Apple. Each item in that breakdown, the counts of results grouped by attribute, is a **facet**.

Facets turn search from a one-dimensional list into something that can help visualize navigation. They set the stage to let you explore results by filtering on structured fields (like *brand* or *price range*) while still searching free text (like *developer laptop*). In other words, faceting is where structured data meets unstructured search.

<Image
    src={faceting}
    className="rounded-xl mx-auto"
  />

Behind the scenes, each facet corresponds to a column or field in your dataset. A search engine computes how many matching documents belong to each unique value or bucket of that field, then returns both the top results and these aggregate counts in a single response. Because a search result usually returns results that are spread across our dataset the database needs a quick way to look up values for each document.

That duality, returning ranked search hits *and* category counts, is what makes faceting so powerful. It’s also what makes it tricky to implement efficiently in a database built for rows, not documents.

## **Faceting in a Row-Based World**

When we set out to design the syntax for faceting, we had three goals:

1. Make it intuitive for both SQL users and Elasticsearch users.
2. Return both the search results and the facet counts in the same payload as fast as possible
3. Integrate cleanly into PostgreSQL’s planner and executor.

We experimented with a few approaches: full `JSONB` result sets, `LATERAL` subqueries, and a behind-the-scenes `UNION`. The one we kept coming back to was a bit of a hack, but a good one. We decided to (ab)use PostgreSQL’s window functions to express faceting instructions, using a JSON DSL that mirrors the [Elasticsearch aggregation API](https://www.elastic.co/docs/explore-analyze/query-filter/aggregations).

That led us to a [new function, `pdb.agg()`](https://docs.paradedb.com/documentation/aggregates/overview), which accepts  `JSONB` configuration that enables a range of aggregations, but most notably for faceting it supports terms (counting unique values), histograms (bucketing unique values), and date_histogram (bucketing timestamps).

Let's say you have a logs table like this:

```sql
CREATE TABLE logs (
    id SERIAL PRIMARY KEY,
    description TEXT,
    category TEXT,
    severity TEXT,
    response_time INTEGER,
    status_code INTEGER,
    timestamp TIMESTAMPTZ
);

```

Then you create a BM25 index that includes the fields you would like use for full-text search (`description` ) alongside those which you’d like to use for faceting or aggregations (any non-text columns, or any columns which use the `literal` tokenizer type.

```sql
CREATE INDEX ON logs USING BM25 (
   id, 
-- this field uses the default tokenizer, and can be used for full-text search
   description,
-- these fields are text, but will only be searched with exact string matching
   category::pdb.literal, severity::pdb.literal, 
-- these fields are not text, and will be loaded into the columnstore
   response_time, status_code, timestamp
);
```

The text columns will be added to the ParadeDB inverted index, while the literal text and non-text columns go into the columnstore (more on this later).

Now you can run faceted search queries that return both ranked results and category counts using `pdb.agg()`:

```sql
SELECT *,
  pdb.agg('{"terms": {"field": "category"}}'::jsonb) OVER () AS categories
FROM logs
WHERE description ||| 'error'
ORDER BY timestamp DESC
LIMIT 10;

```

When you see `OVER ()` alongside an `ORDER` and `LIMIT` (TopN query), it signals: *"compute this aggregation once per search window."*

This design intentionally bridges two paradigms. Elasticsearch users recognize the JSON configuration format, while SQL users see familiar window function syntax. PostgreSQL's parser treats it as a standard window function call, allowing us to implement custom aggregation logic within the familiar SQL framework.

This approach strikes a balance between the expressiveness of Elasticsearch's aggregation API and the readability of SQL window functions.

The first row of the output looks like this:

```sql
id            | 11
description   | API internal server error
severity      | critical
category      | api
response_time | 1500
status_code   | 500
timestamp     | 2024-01-01 10:21:00
categories    | {
  "buckets": [
    {"key": "database", "doc_count": 3},
    {"key": "application", "doc_count": 2},
    {"key": "api", "doc_count": 1},
    {"key": "network", "doc_count": 1}
  ],
  "sum_other_doc_count": 0,
  "doc_count_error_upper_bound": 0
}

```

The faceting information is added alongside the search query using a JSONB column. JSON is the perfect wrapper for the faceting information: it’s structured, self-contained, and easy to parse on the client. Today we attach this to all rows, but in the future, we may optimize this further by returning the facet JSON only on the first row instead of duplicating it.

## **How Faceting Hooks Into the PostgreSQL Planner**

Now we have our syntax, but it isn’t as simple as just writing the `pdb.agg()` function and calling it a day. Because PostgreSQL aggregates (including those in window functions) usually run **after** the data is fetched from indexes and loaded to shared buffers we have a problem. We want to do our search and our faceting in a single pass, but they are executed in fundamentally different places during query execution. 

Luckily Postgres has a backdoor which can help us take control of the query, the very undocumented custom scan API. Using this API we can ask the planner to defer to us, using a custom execution node rather than planning and executing as normal, giving us total control of the query. 

We need to inject this execution node at plan time, intercepting the planning process when we detect a `pdb.agg()` function used as a window. We do this using a planner hook in three stages:

diagram here

1. **Query Interception.** The planner hook scans the query tree for `WindowFunc` nodes using `pdb.agg()`. It replaces them with ParadeDB placeholders *before* PostgreSQL’s `grouping_planner()` runs. This prevents Postgres from creating real `WindowAgg` nodes we’d later need to undo.
2. **Custom Scan Injection.** When queries combine full-text search, `ORDER BY`, `LIMIT`, and our placeholder `WindowAgg` node we take over planning. This is the entry point for Top-N faceting queries. The planner swaps in a `PdbScan` node capable of executing both the search and the aggregation in one pass, making sure that the parts of the query that aren’t handled by ParadeDB (non-search WHERE clauses, outer clauses / CTEs, FILTERS) are delegated back to PostgreSQL as normal.
3. **WindowAgg Extraction**. Extracts and converts the `pdb.agg()` placeholder into a form Tantivy (our underlying search library) understands, adding the information to the custom `PdbScan`. This is where we parse the JSONB definition and build the aggregation tree (terms, range, etc.) to run inside the search engine.

The key insight is timing, this integration happens *before* PostgreSQL attempts standard window function execution. By execution time the window function doesn’t exist and PostgreSQL is running what it sees as a normal custom scan node.

## **How Faceting Executes Inside ParadeDB**

Once PostgreSQL hands off the plan, the real work begins inside ParadeDB’s search layer. At this point, we’re no longer working with rows, we’re working with **posting lists** and **columnar fields** inside Tantivy. The key idea is that ParadeDB executes both **ranking** and **aggregation** in the *same pass* through the index.

diagram here

### **1. Searching and Collecting**

Every faceting query starts by traversing the inverted index using the search criteria from the query. We walk the posting lists for matching terms, answering the search part of the query and producing a stream of matching DocIDs. This stream forms an iterator (Tantivy’s DocSet) representing the candidate documents.

That DocSet is then passed into a **compound collector**, which coordinates all work done over the same stream of documents. This collector runs multiple sub-collectors in parallel, so we can compute different kinds of results (like ranked hits and aggregations) without ever rereading the index.

### **2. TopDocs Collector (ORDER and LIMIT)**

The **TopDocs collector** is responsible for ranking and limiting the result set. The ranking could be by BM25 score (`pdb.score(id`), or it could be by a column on the data (in which case that column will need to be looked up the Tantivy columnstore).

Tantivy uses a quickselect buffer to maintain only the top-N highest ordered documents, traversing the `DocSet` and only keeping N records at any one time using a very small amount of memory.

### **3. Aggregation Collector (Building Facets)**

The Aggregation collector is where the actual faceting magic happens. While TopDocs is busy ordering documents, this collector is busy counting things.

For every matching document, the facet collector fetches and aggregates only the fields you asked for (category, price, rating etc.) directly from Tantivy’s columnstore. This matters because we never need to reconstruct full rows like PostgreSQL would, we can pull just the specific column values needed for aggregation, reducing the amount of buffers we need to read into memory.

Tantivy’s columnstore is designed for exactly this pattern: fast per-document, single-column lookups by offset. It’s not a general-purpose analytics columnstore; it’s optimized for “give me field X for docid Y”, millions of times per second.

Here's where things get interesting from a correctness perspective: this same collector also checks and applies **MVCC filtering** to maintain full ACID guarantees, only aggregating values that are visible to your current transaction.

### **5. Materializing the Final Result**

Once collection finishes, ParadeDB has two outputs:

- The **ranked hits** from the TopDocs collector.
- The **facet aggregation result** from the Aggregation collector.

The Aggregation collector serializes its in-memory facet map into JSON and attaches it as a single facets column next to the search the query output.

## Benchmarking Faceting

So, how does our faceting implementation stack up against other approaches? For a fair comparison we will compare our new faceting against faceting manually using standard Postgres aggregations. The dataset is historic Hackernews posts and comments, which comes in at just under 46million records.

Both approaches will use ParadeDB to power the text-search (not using ParadeDB certainly makes things slower, but that’s not the comparison here), with the manual query pulling all matches through a common table expression (CTE) and aggregating as we go.

Our new query look like this:

```sql
SELECT
  id, title, score, pdb.score(id) AS rank,
  pdb.agg('{"histogram": {"field": "score", "interval": 50}}') 
    OVER () as facets
FROM hn_items
WHERE (text ||| 'postgresql')
ORDER BY rank DESC
LIMIT 5;
```

While our manual query looks like this:

```sql
WITH 
-- get the id, score and bm25 rank for the search query. Leave the other
-- columns to join back after we LIMIT to speed things up
hits AS  (
  SELECT         
    id,
    pdb.score(id) AS rank,                                                 
    score
  FROM hn_items              
  WHERE (text ||| 'postgresql')
),
-- do our faceting on the full result-set based on score and build up a 
-- JSON object
facets AS (
  SELECT jsonb_build_object(
           'buckets',
           jsonb_agg(
             jsonb_build_object('key', score_bucket, 'doc_count', cnt)
             ORDER BY score_bucket
           )
         ) AS facets
  FROM (
    SELECT
    -- bucket the score by 50
      (score / 50) * 50 AS score_bucket,
      COUNT(*) AS cnt
    FROM hits
    GROUP BY score_bucket
  ) t
),
-- order the orginal hits by rank and only return the first 10
limited AS (
  SELECT id, rank FROM hits ORDER BY rank DESC LIMIT 10
)
-- join the limited hits back to the hits to get the titles (without this
-- optimization the query is much slower)
-- then join the facets to each row.
SELECT h.id, title, score ,facets
FROM limited h
JOIN hn_items i on i.id = h.id
CROSS JOIN facets f;
```

That’s a bit of a mouthful, but shows the kind of hoops you needed to jump through to do faceting manually. While this query hits around the same number of `shared_buffers` as the first query we will see that it has very different performance characteristics as the number of results increases. 

<FacetingChart />



## Conclusion
