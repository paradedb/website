import Image from "next/image";
import { ReferenceLine } from "recharts";
import { BarChart, Card, Subtitle, Bold } from "@tremor/react"
import FacetingChart from "./FacetingChart"
import heroImage from "./images/hero.png"

import blogMetadata from "./metadata.json";
import { AuthorSection } from "@/components/AuthorSection";
import { HeroImage } from "@/components/HeroImage";
import { Title } from "@/components/Title";
import { Note } from "@mintlify/components"
import faceting from "./images/demandsciencesmall.png";

<Title metadata={blogMetadata} />
<AuthorSection metadata={blogMetadata} />
<HeroImage src={heroImage} metadata={blogMetadata} />

Picture this: you've built a perfectly reasonable search app. Users type "dinosaur," get results, everybody's happy. Then someone asks, "but how many of these are carnivores?" So you add filters. Then they want to know how many results each filter would give them *before* they click it. Welcome to ~Jurrasic park~ faceted search.

In this post, we'll break down how we brought **Elasticsearch-style faceting directly into PostgreSQL;** the syntax, the planner integration, and the execution strategy that makes it possible. We'll show you how we structured the SQL API using window functions, and the results we can get from pushing as much work as possible down into our underlying search library (spoiler alert: [over an order of magnitude faster](#performance-why-this-matters) for large result sets).

<Note>
If you aren't familiar with [ParadeDB](https://paradedb.com), we're a Postgres extension that brings modern search capabilities directly into your database. Instead of managing separate search infrastructure, you get BM25 full-text search, vector search, and real-time analytics while keeping all your data in Postgres with full ACID guarantees.
</Note>

## How Search Engines Count

Faceting is a way of summarizing the contents of a search result. When you type a query like "laptop", you’re not just asking for a list of matching documents, you’re also thinking, *what kinds of laptops exist in this result set?* Maybe there are 1,200 Dell laptops, 800 Lenovo, and 600 Apple. Each item in that breakdown, the counts of results grouped by attribute, is a **facet**.

Facets turn search from a one-dimensional list into something that can help visualize navigation. They set the stage to let you explore results by filtering on structured fields (like *brand* or *price range*) while still searching free text (like *developer laptop*). In other words, faceting is where structured data meets unstructured search.

<figure className="flex flex-col items-center">
  <Image
    src={faceting}
    alt="Faceted search powering real-world insights: an example from our customer, DemandScience."
    className="rounded-xl"
  />
  <figcaption className="mt-2 text-sm text-gray-500">
    Faceted search powering real-world insights: an example from our customer, DemandScience.
  </figcaption>
</figure>

Behind the scenes, each facet corresponds to a column or field in your dataset. A search engine computes how many matching documents belong to each unique value or bucket of that field, then returns both the top results and these aggregate counts in a single response. Because a search result usually returns results that are spread across our dataset the database needs a quick way to look up values for each document.

That duality, returning ranked search hits *and* category counts, is what makes faceting so powerful. It's also what makes it tricky to implement efficiently in a database built for rows, not documents.

## The Problem with Traditional Approaches

If you're thinking, "I'll just run a few extra COUNT queries, how hard can it be?", you're about to discover why search engines exist in the first place.

The thing is, faceting *looks* simple: it's just grouping and counting. But try to make it fast in a traditional row-based database, and you'll run into serious performance challenges. Want to show search results *and* category counts from a single query? That's either two index scans or a full index scan and a lot of data transferred. Want it to be responsive? That becomes increasingly difficult as the number of results increases.

But before you even hit performance issues, you need to get your syntax sorted.

## ParadeDB's Solution: Window Function Syntax

ParadeDB solves this by bringing faceting directly into PostgreSQL using familiar window function syntax. Instead of complex CTEs and multiple queries, you get both search results and facet counts in a single, clean query:

```sql
SELECT
  id, title, score, pdb.score(id) AS rank,
  pdb.agg('{"histogram": {"field": "score", "interval": 50}}') 
    OVER () as facets
FROM hn_items
WHERE (text ||| 'postgresql')
ORDER BY rank DESC
LIMIT 5;
```

The magic in the `OVER ()` syntax: this means "over the entire result set, ignoring the LIMIT", whether you're using ParadeDB, Snowflake, or any SQL-compliant database. This window syntax computes both results in a single pass through the data.

We use a JSON DSL that mirrors the [Elasticsearch aggregation API](https://www.elastic.co/docs/explore-analyze/query-filter/aggregations), with our [`pdb.agg()` function](https://docs.paradedb.com/documentation/aggregates/overview) supporting [terms](https://docs.paradedb.com/documentation/aggregates/bucket/terms) (counting unique values), [histograms](https://docs.paradedb.com/documentation/aggregates/bucket/histogram) (bucketing unique values), and [date_histogram](https://docs.paradedb.com/documentation/aggregates/bucket/datehistogram) (bucketing timestamps).

The result includes both your search hits and the faceting data as a JSONB column. The faceting information appears alongside each row, giving you structured, self-contained aggregation results that are easy to parse on the client side. 

### MVCC and Performance Tradeoffs

By default, ParadeDB maintains full ACID guarantees by checking transaction visibility for every aggregated document. However, for large result sets where approximate counts are acceptable, you can disable MVCC checking for significant performance gains:

```sql
-- Default: MVCC enabled (ACID compliant, slower)
pdb.agg('{"terms": {"field": "category"}}') OVER ()

-- MVCC disabled: ~3x faster for large result sets
pdb.agg('{"terms": {"field": "category"}}', false) OVER ()
```

When MVCC is disabled, the aggregation operates directly on the search index without visibility checks. This is useful for analytics workloads on large datasets where some lag in reflecting the latest udpates is acceptable. We're still the transactional Elasticsearch, this is just an optional optimization when you need maximum performance.

## Comparing the Approaches

Let's see how this compares to traditional PostgreSQL approaches. When you need faceted search in traditional PostgreSQL, you end up writing complex queries with CTEs and manual aggregations. Here's what a very optimized version of "manual faceting" looks like:

```sql
WITH 
-- get the id, score and bm25 rank for the search query
hits AS  (
  SELECT         
    id,
    pdb.score(id) AS rank,                                                 
    score
  FROM hn_items              
  WHERE (text ||| 'postgresql')
),
-- do our faceting on the full result-set and build up a JSON object
facets AS (
  SELECT jsonb_build_object(
           'buckets',
           jsonb_agg(
             jsonb_build_object('key', score_bucket, 'doc_count', cnt)
             ORDER BY score_bucket
           )
         ) AS facets
  FROM (
    SELECT
      (score / 50) * 50 AS score_bucket,
      COUNT(*) AS cnt
    FROM hits
    GROUP BY score_bucket
  ) t
),
-- order the original hits by rank and return the top results
limited AS (
  SELECT id, rank FROM hits ORDER BY rank DESC LIMIT 10
)
-- join everything together
SELECT h.id, title, score ,facets
FROM limited h
JOIN hn_items i on i.id = h.id
CROSS JOIN facets f;
```

Here's what **ParadeDB faceting** looks like for the same result:

```sql
SELECT
  id, title, score, pdb.score(id) AS rank,
  pdb.agg('{"histogram": {"field": "score", "interval": 50}}') 
    OVER () as facets
FROM hn_items
WHERE (text ||| 'postgresql')
ORDER BY rank DESC
LIMIT 5;
```

Same result, simpler syntax, and dramatically better performance. 

## **Performance: Why This Matters**

We benchmarked both approaches against a dataset of 46 million Hacker News posts and comments. The results show the advantage of our integrated faceting approach:

<FacetingChart />

As the chart shows, manual faceting performance degrades significantly with larger result sets, while ParadeDB's faceting maintains consistent performance by executing both search and aggregation in a single pass through the index. The orange line shows ParadeDB with MVCC disabled, demonstrating the additional performance gains possible when strict transactional consistency isn't required. At scale, this represents over an order of magnitude improvement over traditional approaches.

## **How We Built It: Technical Implementation**

When we set out to design this syntax, we had three goals:

1. Make it intuitive for both SQL users and Elasticsearch users.
2. Return both the search results and the facet counts in the same payload as fast as possible
3. Integrate cleanly into PostgreSQL's planner and executor.

Let's walk through how this works with a concrete example. Say you have a logs table like this:

```sql
CREATE TABLE logs (
    id SERIAL PRIMARY KEY,
    description TEXT,
    category TEXT,
    severity TEXT,
    response_time INTEGER,
    status_code INTEGER,
    timestamp TIMESTAMPTZ
);
```

After creating a ParadeDB search index[^1], you can run faceted search queries that return both ranked results and category counts using `pdb.agg()`:

```sql
SELECT *,
  pdb.agg('{"terms": {"field": "category"}}'::jsonb) OVER () AS categories
FROM logs
WHERE description ||| 'error'
ORDER BY timestamp DESC
LIMIT 10;

```

When you see `OVER ()` alongside an `ORDER` and `LIMIT` (TopN query), it signals: *"compute this aggregation once per search window."*

This design intentionally bridges two paradigms. Elasticsearch users recognize the JSON configuration format, while SQL users see familiar window function syntax. PostgreSQL's parser treats it as a standard window function call, allowing us to implement custom aggregation logic within the familiar SQL framework.

This approach strikes a balance between the expressiveness of Elasticsearch's aggregation API and the readability of SQL window functions.

The first row of the output looks like this:

```sql
id            | 11
description   | API internal server error
severity      | critical
category      | api
response_time | 1500
status_code   | 500
timestamp     | 2024-01-01 10:21:00
categories    | {
  "buckets": [
    {"key": "database", "doc_count": 3},
    {"key": "application", "doc_count": 2},
    {"key": "api", "doc_count": 1},
    {"key": "network", "doc_count": 1}
  ],
  "sum_other_doc_count": 0,
  "doc_count_error_upper_bound": 0
}

```

The faceting information is added alongside the search query using a JSONB column. JSON is the perfect wrapper for the faceting information: it’s structured, self-contained, and easy to parse on the client. Today we attach this to all rows, but in the future, we may optimize this further by returning the facet JSON only on the first row instead of duplicating it.

## **How Faceting Hooks Into the PostgreSQL Planner**

Now we have our syntax, but it isn’t as simple as just writing the `pdb.agg()` function and calling it a day. Because PostgreSQL aggregates (including those in window functions) usually run **after** the data is fetched from indexes and loaded to shared buffers we have a problem. We want to do our search and our faceting in a single pass, but they are executed in fundamentally different places during query execution. 

PostgreSQL provides a custom scan API that allows extensions to implement their own execution nodes. However, to fully integrate faceting with window functions, we also need to use planner hooks to intercept the planning process before PostgreSQL creates standard window aggregation nodes.

We inject our custom execution node at plan time, intercepting the planning process when we detect a `pdb.agg()` function used as a window. We do this using a combination of planner hooks and the custom scan API in three stages:

diagram here

1. **Query Interception.** The planner hook scans the query tree for `WindowFunc` nodes using `pdb.agg()`. It replaces them with ParadeDB placeholders *before* PostgreSQL’s `grouping_planner()` runs. This prevents Postgres from creating real `WindowAgg` nodes we’d later need to undo.
2. **Custom Scan Injection.** When queries combine full-text search, `ORDER BY`, `LIMIT`, and our placeholder `WindowAgg` node we take over planning. This is the entry point for Top-N faceting queries. The planner swaps in a `PdbScan` node capable of executing both the search and the aggregation in one pass, making sure that the parts of the query that aren’t handled by ParadeDB (non-search WHERE clauses, outer clauses / CTEs, FILTERS) are delegated back to PostgreSQL as normal.
3. **WindowAgg Extraction**. Extracts and converts the `pdb.agg()` placeholder into a form Tantivy (our underlying search library) understands, adding the information to the custom `PdbScan`. This is where we parse the JSONB definition and build the aggregation tree (terms, range, etc.) to run inside the search engine.

The key insight is timing, this integration happens *before* PostgreSQL attempts standard window function execution. By execution time the window function doesn’t exist and PostgreSQL is running what it sees as a normal custom scan node.

## **How Faceting Executes Inside ParadeDB**

Once PostgreSQL hands off the plan, the real work begins inside ParadeDB's search layer. At this point, we're no longer working with rows, we're working with **search indexes** and **column data** inside Tantivy. The key idea is that ParadeDB executes both **ranking** and **aggregation** in the *same pass* through the index.

diagram here

### **1. Searching and Collecting**

Every faceting query starts by traversing the search index using the search criteria from the query. We walk through the index entries for matching terms, answering the search part of the query and producing a stream of matching document IDs. This stream forms an iterator representing the candidate documents.

This document stream is then passed into a **compound collector**, which coordinates all work done over the same stream of documents. This collector runs multiple sub-collectors in parallel, so we can compute different kinds of results (like ranked hits and aggregations) without ever rereading the index.

### **2. TopDocs Collector (ORDER and LIMIT)**

The **TopDocs collector** is responsible for ranking and limiting the result set. The ranking could be by BM25 score (`pdb.score(id`), or it could be by a column value (which gets looked up from the column storage).

Tantivy uses a quickselect buffer to maintain only the top-N highest ordered documents, traversing the document stream and only keeping N records at any one time using a very small amount of memory.

### **3. Aggregation Collector (Building Facets)**

The Aggregation collector is where the actual faceting magic happens. While TopDocs is busy ordering documents, this collector is busy counting things.

For every matching document, the facet collector fetches and aggregates only the fields you asked for (category, price, rating etc.) directly from Tantivy's column storage. This matters because we never need to reconstruct full rows like PostgreSQL would, we can pull just the specific column values needed for aggregation, reducing the amount of data we need to read into memory.

Tantivy's column storage is designed for exactly this pattern: fast per-document, single-column lookups. For string fields, it uses dictionary encoding, so the entire aggregation happens on integers rather than strings, which are only converted back to strings at the very end. This makes the aggregation process extremely efficient - we're counting integer IDs rather than comparing and hashing string values millions of times per second.

Here's where things get interesting from a correctness perspective: this same collector also checks and applies **MVCC filtering** to maintain full ACID guarantees, only aggregating values that are visible to your current transaction. When the MVCC flag is set to `false`, we skip these visibility checks entirely for maximum performance—useful for analytics workloads where approximate counts are acceptable.

### **5. Materializing the Final Result**

Once collection finishes, ParadeDB has two outputs:

- The **ranked hits** from the TopDocs collector.
- The **facet aggregation result** from the Aggregation collector.

The Aggregation collector serializes its in-memory facet map into JSON and attaches it as a single facets column next to the search the query output.


## Conclusion

Faceting is one of those features that looks simple on the surface but gets complex fast when you try to implement it efficiently in a traditional row-based database. The performance overhead of manual faceting approaches scales poorly with result set size, forcing developers to choose between feature richness and performance.

ParadeDB's approach removes that complexity by bringing Elasticsearch-style faceting directly into PostgreSQL. By integrating faceting at the search engine level, we can execute both ranking and aggregation in a single pass through the index, delivering order-of-magnitude performance improvements without forcing you to manage a separate search infrastructure.

The result is a simple SQL interface that gives you the best of both worlds: PostgreSQL's ACID guarantees and transactional semantics, combined with the performance and developer experience of modern search engines.

Ready to try faceted search in your PostgreSQL database? [Get started with ParadeDB](https://docs.paradedb.com/documentation/getting-started) and see how faceting can transform your search experience.

[^1]: To create a ParadeDB search index, you would run: `CREATE INDEX ON logs USING BM25 (id, description, category::pdb.literal, severity::pdb.literal, response_time, status_code, timestamp);` Full documentation on index creation can be found [here](https://docs.paradedb.com/documentation/indexing/create-index).
