import Image from "next/image";
import blogMetadata from "./metadata.json";
import { AuthorSection } from "@/components/AuthorSection";
import { HeroImage } from "@/components/HeroImage";
import { Title } from "@/components/Title";
import heroImage from "./images/hero.png";
import reranking from "./images/reranking.png";

<Title metadata={blogMetadata} />
<AuthorSection metadata={blogMetadata} />
<HeroImage src={heroImage} metadata={blogMetadata} />

When I search for "King", I’m looking for LOTR- _The Return of the King_. You might be looking for _The King's Speech_. A generic search engine returns the same results for both of us, likely a mix of Fantasy and Drama, which means it fails both of us.

Personalization can take a "good enough" search experience to a magical experience. Traditionally, building personalized search involved a complex Rube Goldberg machine: syncing data between a primary database and a search engine (like Elasticsearch), building a separate ML recommendation service, dragging data across the wire to Python workers for re-ranking, and then stitching it all back together. It works, but the cost is high: architectural complexity, network latency, and data synchronization nightmares.

This isn't a new concept—I first presented this approach at [PGConf NYC 2023](https://postgresql.us/events/pgconfnyc2023/schedule/session/1412-postgres-as-personalization-engine/), where I spoke about using Postgres as a personalization engine, and today we'll build it in standard Postgres so you can too.

What if you could build a FAANG-quality personalized search engine _entirely_ within Postgres? No new infrastructure. No network hops. Just SQL.

In this post, we’ll build a personalized movie search engine using **ParadeDB**. We’ll implement a "Retrieve and Rerank" pipeline that combines the speed of BM25 full-text search with the intelligence of vector-based personalization, all without the data ever leaving PostgreSQL.

This architectural simplicity is a superpower. Having [engineered search infrastructure systems at **Instacart**](https://tech.instacart.com/how-instacart-built-a-modern-search-infrastructure-on-postgres-c528fa601d54), I've seen firsthand how "Rube Goldberg" pipelines crumble under their own weight. Similar patterns power search at giants like [Target](https://cloud.google.com/blog/topics/retail/from-query-to-cart-inside-targets-search-bar-overhaul-with-alloydb-ai) and [Alibaba](https://www.alibabacloud.com/blog/learn-how-alibaba-engineers-accommodated-for-face-masks-in-their-face-recognition-algorithms_596040), and today, we'll build it in standard Postgres.

## The Architecture: Retrieve & Rerank

**Retrieve and Rerank approach** breaks the problem into two stages, utilizing the strengths of different algorithms:

1. **Retrieval** We use [**BM25**](https://www.paradedb.com/learn/search-concepts/bm25) (standard full-text search) for lexical search to find the top N candidates matching the user's query. This is extremely fast and computationally cheap, filtering millions of rows down to a relevant subset. You can [combine BM25 and Pgvector for a hybrid retrieval](https://www.paradedb.com/learn/search-concepts/hybrid-search).
2. **Reranking** : We use Cosine Similarity and [semantic / vector search](https://www.paradedb.com/learn/search-concepts/vector-search) on just those 100 candidates to re-order them based on the user's personal profile.

Why this approach? Because running vector similarity search on your _entire_ dataset for every query is overkill. For a n=100 items (or even 1000), we would not need any vector index and instead brute force our way out.

## The Intuition: Vectors Match Vibes

Before we look at code, it helps to understand _why_ this works.

<Image
    src={reranking}
    alt="tbc"

/>

Traditional search matches **keywords**. If you search for "Sci-Fi", a database looks for the string "Sci-Fi". To a keyword engine, _Star Wars_ (Action/Adventure) and _Solaris_ (Psychological Drama) are identical because they share a tag.

A **Uer Vector** represents aggregated taste. It can be built from **explicit signals** (like the 5-star ratings we look at in this post) or **implicit signals** (like clickstream data or watch time).

In our example, if a user explicitly rates The king’s speech pooly (Dislike) but rate _Star Wars_ highly, we mathematically steer the user’s vector _towards_ the fantasy region in movie embedding space and _away_ from the drama region.

## Implementation

Let's build it. We need two core entities: **Movies** (the content) and **Users** (the personalization context).

### 1. Core Schema

We use standard Postgres tables for movies and users. These are core tables. We also have ratings table that contains user’s likes and dislikes. For success in personalization you do need some user specific signals that can be modeled into their profiles. Here we use explicit likes/dislikes in the form of ratings. A rating of 4 or 5 signals a like, a rating of 1 and 2 signals a dislike. Implicit signals like watching a video, interacting with content, duration of engagements can be used as well.

```sql
-- Enable required extensions
CREATE EXTENSION IF NOT EXISTS pg_search;
CREATE EXTENSION IF NOT EXISTS vector;

-- The content we want to search
CREATE TABLE movies (
    movie_id BIGINT PRIMARY KEY,
    title TEXT NOT NULL,
    genres TEXT[],
    -- Embedding representing the movie's semantic content (e.g. from OpenRouter/HuggingFace) We used a small model all-MiniLM-L12-v2 that has 384 dims
    content_embedding vector(384)
);

-- The user profiles
CREATE TABLE users (
    user_id BIGINT PRIMARY KEY,
    -- A vector representing the user's taste
    embedding vector(384)
);

```

Movies get a ParadeDB’s BM25 index.

```sql
CREATE INDEX movies_search_idx ON movies
  USING bm25 (movie_id, title, year, imdb_id, tmdb_id, genres)
  WITH (key_field='movie_id');

```

We use Pgvector’s vector column to store movie and users embeddings. Movie embeddings can be created by calling OpenRouter or Huggingface APIs. For the demo we used `sentence-transformers/all-minilm-l12-v2` model.

### 2. Generating User Profiles

How do we get a vector that represents a user's taste? In a complex stack, you'd have a background worker processing datastreams in Python. We can do that too, but since all our movie embeddings are already being streamed into postgres we can use standard SQL aggregation to get user embeddings.

```sql
-- Generate a user's profile vector:
-- Move towards movies they like (>= 4 stars)
-- Move AWAY from movies they dislike (< 3 stars)
UPDATE users SET embedding = (
    SELECT SUM(
        CASE
            WHEN r.rating >= 4.0 THEN m.content_embedding
            ELSE m.content_embedding * -1
        END
    )
    FROM ratings r
    JOIN movies m ON r.movie_id = m.movie_id
    WHERE r.user_id = users.user_id
    AND (r.rating >= 4.0 OR r.rating < 3.0) -- Ignore mediocre ratings
);

```

This is the power of "In-Database AI". You don't need a separate pipeline to keep user profiles up to date. You can update them transactionally as ratings come in.

### 3. The Unified Search and Recommender Engine

Now for the main event. We can execute the entire **Retrieve & Rerank** pipeline in a single SQL query using Common Table Expressions (CTEs).

Instead of a black box, let's break this down into four logical steps: **Retrieve**, **Normalize**, **Personalize**, and **Fuse**.

### Step 1: The Retrieval (Fast)

First, we use BM25 to find the top 100 candidates. This is our "coarse filter"—it casts a wide net to find anything relevant to the text query.

```sql
WITH first_pass_retrieval AS (
    SELECT
        movie_id,
        title,
        paradedb.score(movie_id) as bm25_score
    FROM movies
    WHERE title @@@ 'action'
    ORDER BY bm25_score DESC
    LIMIT 100
),

```

### Step 2: The Normalization (Math)

BM25 scores are unbounded (they can be 10.5 or 100.2), while Vector Cosine Similarity is always between -1 and 1. To combine them fairly, we normalize the BM25 scores to a 0-1 range using a standard Min-Max scaler.

```sql
normalization AS (
    SELECT
        *,
        (bm25_score - MIN(bm25_score) OVER()) /
        NULLIF(MAX(bm25_score) OVER() - MIN(bm25_score) OVER(), 0) as normalized_bm25
    FROM first_pass_retrieval
),

```

### Step 3: The Personalization (Smart)

Now we bring in the user context. We calculate the similarity between our _normalized_ candidates and the current user's taste profile.

```sql
personalized_ranking AS (
    SELECT
        n.movie_id,
        n.title,
        n.normalized_bm25,
        -- Cosine similarity: 1 - cosine_distance transforms distance to a 0-1 similarity score
        (1 - (u.embedding <=> m.content_embedding)) as personal_score
    FROM normalization n
    JOIN movies m ON n.movie_id = m.movie_id
    CROSS JOIN users u
    WHERE u.user_id = 12345 -- The Current User
)

```

### Step 4: The Fusion (Final Polish)

Finally, we combine the signals. We take a weighted average: 50% for the text match (relevance) and 50% for the user match (personalization).

```sql
SELECT
    title,
    (0.5 * normalized_bm25 + 0.5 * personal_score) as final_score
FROM personalized_ranking
ORDER BY final_score DESC
LIMIT 10;

```

This composable approach allows you to tune the knobs. Want personalization to matter more? Change the `0.5` weight. Want to filter candidates differently? Change the `first_pass` CTE. It's just SQL.

## Other Recommender Engine Workloads

There are few more workloads that can be supported in a similar way inside Postgres.

1. Collaborative filtering → **Users Like Me**
   1. Find other users who are similar to a user id. This can be directly used or used as subquery to find `items liked by similar users`.
2. Real time Updates via Triggers
   1. Whenever a users’ behaviour changes, a new rating arrives a trigger can update user’s embeddings in real time.

## Trade-offs: The Landscape

No architecture is perfect. While the "In-Database" approach offers unbeatable simplicity, it helps to understand where it sits in the broader landscape of personalization strategies.

### 1. In Application Layer Inference (Python/Node.js)

The most common alternative is fetching data and re-ranking it in your application code (or a dedicated microservice).

- **Pro**: **Maximum Flexibility**. You can use arbitrary logic and complex machine learning models (PyTorch/TensorFlow) that might be hard to express in SQL. (Though note: standard A/B testing is still easy in Postgres—you just generate different SQL queries or swap the "User Embedding" table in your application logic).
- **Con**: **The "Data Transfer Tax"**. You have to ship thousands of candidate rows over the network to your application just to discard most of them. This adds significant latency and serialization overhead.

### 2. Dedicated Inference Platforms (e.g. Ray Serve, NVIDIA Triton)

For the most advanced use cases, companies perform retrieval in a database and then send candidate IDs to a dedicated inference cluster running models like BERT or DLRM.

- **Pro**: **State-of-the-Art Accuracy**. You can run massive, GPU-accelerated deep learning models that are too heavy for a standard database.
- **Con**: **Infrastructure Sprawl**. You are now managing a database _and_ a high-availability ML cluster. You also pay a latency penalty for every request as data hops from disk -> app -> inference server -> app.

### 3. The "Cross-Encoder" Approach (e.g. Cohere Re-ranker)

### The ParadeDB Sweet Spot

A powerful alternative is using a **Cross-Encoder**. While our in-database approach uses a **Bi-Encoder.** A bi-encoder is where we calculate vector embeddings for the Query and Document _independently_ and just look at the angle/distance-angle between them, a Cross-Encoder feeds _both_ the query and the document into the model simultaneously.

- **Pro**: **Maximum Accuracy**. Cross-encoders are more advanced because they understand the interaction between words in the query and words in the document. Although they are generally considered blackbox for most users
- **Con**: **The Latency & Cost Cliff**. You cannot pre-compute a Cross-Encoder score. You must run this heavy model _at query time (inference time)_ for every single candidate. This typically adds 200ms–500ms of latency per search and requires sending your sensitive user data to a third-party API.

**When to use which?**

- **Use in database Bi-Encoder**: For most use cases where you want "Good Enough" and fast personalization (&lt;100ms latency), data privacy, and zero operational complexity. Having no network hop reduces distributed systems issues like high tail latencies.
- **Use Cross-Encoder approach**: For the 5% of use cases where squeezing out the final drops of relevance is worth the extra latency, vendor cost, and privacy trade-offs.

The architecture we built at ParadeDB hits a "Sweet Spot" for 95% of use cases. It gives you **hybrid search** and **personalization** with **zero added infrastructure**. You aren't managing separate inference clusters (like Ray), paying for Re-ranker API calls, or maintaining ETL pipelines. You just have Postgres.

The main consideration? **Resource Management**. Because search runs on your database, it shares CPU and RAM with your transactional workload. For most applications, this is a non-issue given Postgres's efficiency. For high-scale deployments, you can simply run this on a **dedicated read replica** to isolate inference load from your primary writes.

## Conclusion

By pushing this logic into the database, we achieve **Simplicity**.

There is no "synchronization lag" where a user rates a movie but their recommendations don't update for an hour. There is no fragile ETL pipeline to debug when recommendations look weird. There is no network latency adding 50ms to every search request while data travels to a ranker.

This pattern isn't just a PostgreSQL trick; it's a fundamental optimization principle called **Compute Pushdown**. You see it everywhere in high-performance computing:

- **Big Data**: Modern data warehouses like BigQuery and Snowflake push filtering logic down to the storage layer to avoid scanning petabytes of data.
- **Edge Computing**: IoT devices process sensor data locally (at the "edge") rather than sending every raw byte to the cloud, saving massive bandwidth.

ParadeDB applies this same principle to search: **Move the compute to the data, not the data to the compute.** By treating personalization as a database query rather than an application workflow, we simplify the stack, reduce latency, and give you a unified engine for both search and recommendations. If there is any more recommender engine workload you would like to see in Postgres, do write to us link to paradedb community.

---

_This post is based on a [functional prototype concept](https://github.com/ankitml/paradedb-reranker) demonstrating in-database personalization patterns._
