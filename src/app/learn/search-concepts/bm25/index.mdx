import learnMetadata from "./metadata.json";
import { AuthorSection } from "@/components/AuthorSection";
import { Title } from "@/components/Title";
import { Note } from "@mintlify/components";
import Image from "next/image";

<Title metadata={learnMetadata} />
<AuthorSection metadata={learnMetadata} />

[Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25) is a **ranking function** used in search engines to score documents by relevance to a query. The BM is short for `best matching`, and `25` is the version of the function that worked best in testing. It improves on earlier methods like [TF-IDF ](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) by accounting for document length and diminishing returns from repeated terms. It powers systems like [Elasticsearch](https://github.com/elastic/elasticsearch), [Solr](https://github.com/apache/solr), and Postgres extensions like [pg_search](https://github.com/paradedb/paradedb).

Even though it's over 30 years old, BM25 remains the default ranking algorithm in most search engines because it’s simple, explainable, and performs well in practice.

## How BM25 Works

It's important to note that BM25 is what’s known as a bag-of-words model. It doesn’t look at term order, phrase structure, or proximity: only at which words appear and how frequently they occur.

BM25 starts by breaking the search query into individual terms. For each term, it scores how well every document matches based on three main signals:

- **Term frequency (TF):** documents that mention a query term more often score higher.
- **Inverse document frequency (IDF):** rare terms are weighted more than common ones.
- **Document length normalization:** shorter documents are preferred to long ones that mention the term in passing.

Each term contributes its own score, and BM25 sums those scores to produce an overall relevance value for the document.

In simplified form, BM25 can be thought of as:

$$
\text{score} = \sum \text{IDF} \times \text{adjusted\_term\_frequency}
$$

where the adjusted term frequency reduces the impact of very frequent terms and normalizes for document length.

<Note>
  The full BM25 formula is slightly more complex. It includes tunable parameters
  $k₁$ (how quickly term frequency saturates) and $b$ (how strongly document
  length is normalized). These controls make BM25 adaptable across different
  datasets and document types.
</Note>

## Why BM25 Works Well

BM25 has three key strengths which have allowed it to stay relevant for so long:

- **Simplicity:** easy to understand and implement.
- **Efficiency:** fast enough to run in real-time over large datasets.
- **Explainability:** each part of the score can be traced to a clear factor.

Because of these traits, BM25 remains the baseline for relevance in modern search systems.

## When to Use BM25

Use BM25 when:

- You’re building keyword-based or full-text search.
- You need explainable and consistent ranking.
- You need instant responses to search or auto-completion queries.

Suppliment BM25 wth another approach when:

- You need semantic or meaning-based retrieval.

Many production systems combine BM25 with vector search or neural re-ranking to balance precision and semantic understanding. Over time BM25 has been proven to be the perfect base for almost any search application.

## Example: Scoring a Query with BM25

Imagine a user searches for **"inverted index"**.

BM25 first breaks the query into two terms: **“inverted”** and **“index.”**

Then, for each document, it scores how well those terms match:

| **Document** | **Length** | **Term counts**            | **TF–IDF signals**                                          | **Relative Score** |
| ------------ | ---------- | -------------------------- | ----------------------------------------------------------- | ------------------ |
| **Doc A**    | 60 words   | inverted: 1<br />index: 1  | rare → high IDF<br />common → low IDF                       | **Higher**         |
| **Doc B**    | 200 words  | inverted: 1<br />index: 10 | rare → high IDF<br />common → low IDF<br />+ length penalty | **Lower**          |

Even though _Doc B_ repeats “index” more times, BM25 favors _Doc A_ because:

1. “inverted” is a rare term with higher IDF,
2. both documents mention it once (similar TF), and
3. _Doc A_ is shorter, so its matches are more concentrated.

**BM25 rewards focused, relevant mentions of rare terms, not repetition in long text.**

## Summary

BM25 remains the standard ranking algorithm for keyword search.

It combines frequency, rarity, and length normalization into a single scoring model that quickly produces balanced and predictable results.

For most full-text search applications BM25 is the right place to start.
